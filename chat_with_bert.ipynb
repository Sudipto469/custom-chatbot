{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b69638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81441fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b27c3ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('train_data.csv')\n",
    "df.columns= ['text','label']\n",
    "print(len(train))\n",
    "# df.head()\n",
    "# df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "818f4bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15    0.05\n",
       "12    0.05\n",
       "17    0.05\n",
       "11    0.05\n",
       "2     0.05\n",
       "5     0.05\n",
       "8     0.05\n",
       "14    0.05\n",
       "0     0.05\n",
       "18    0.05\n",
       "1     0.05\n",
       "6     0.05\n",
       "3     0.05\n",
       "13    0.05\n",
       "16    0.05\n",
       "4     0.05\n",
       "19    0.05\n",
       "9     0.05\n",
       "10    0.05\n",
       "7     0.05\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the labels into encodings\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['label'])\n",
    "# check class distribution\n",
    "df['label'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "197af3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example we have used all the utterances for training purpose\n",
    "train_text, train_labels = df['text'], df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00c464f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6e785ebeb64f9db86be7dfada4a291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SUDIPTO DAS\\AppData\\Roaming\\Python\\Python37\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\SUDIPTO DAS\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed9687d43f4e44c8904836562278fce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a006305cbc4163832ec03b26a2aa98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c9a7d04f2b4db8ae8f37d4777b7381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c51b257dcaff4ff3b78ef5f8f5adc1c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, BertTokenizerFast\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "# Import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19ff2d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99683d3d7ec84eee9ef4a971cd47b6c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f79314fdebfa49b9b586ac392ecc04a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe9ff81165844d599092eb9bffc9794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21afddcf253e4ef2a75868251a044018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "# Load the DistilBert tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# Import the DistilBert pretrained model\n",
    "bert = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f363122a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2023,  2003,  1037,  4487, 16643,  2140, 14324,  2944,  1012,\n",
      "           102],\n",
      "        [  101,  2951,  2003,  3514,   102,     0,     0,     0,     0,     0,\n",
      "             0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "text = [\"this is a distil bert model.\",\"data is oil\"]\n",
    "# Encode the text\n",
    "encoded_input = tokenizer(text, padding=True,truncation=True, return_tensors='pt')\n",
    "print(encoded_input)\n",
    "# In input_ids:\n",
    "# 101 - Indicates beginning of the sentence\n",
    "# 102 - Indicates end of the sentence\n",
    "# In attention_mask:\n",
    "# 1 - Actual token\n",
    "# 0 - Padded token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b0b9beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SUDIPTO DAS\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\tokenization_utils_base.py:2360: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Based on the histogram we are selecting the max len as 8\n",
    "max_seq_len = 8\n",
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer(\n",
    "    train_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ba07123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train set\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "134862e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "#define a batch size\n",
    "batch_size = 16\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "# DataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "14e70446",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "    def __init__(self, bert):      \n",
    "        super(BERT_Arch, self).__init__()\n",
    "        self.bert = bert \n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "        # dense layer\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "        self.fc2 = nn.Linear(512,256)\n",
    "        self.fc3 = nn.Linear(256,20)\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "        #pass the inputs to the model  \n",
    "        cls_hs = self.bert(sent_id, attention_mask=mask)[0][:,0]\n",
    "\n",
    "        x = self.fc1(cls_hs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        # output layer\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c1614ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "BERT_Arch                                               --\n",
       "├─DistilBertModel: 1-1                                  --\n",
       "│    └─Embeddings: 2-1                                  --\n",
       "│    │    └─Embedding: 3-1                              (23,440,896)\n",
       "│    │    └─Embedding: 3-2                              (393,216)\n",
       "│    │    └─LayerNorm: 3-3                              (1,536)\n",
       "│    │    └─Dropout: 3-4                                --\n",
       "│    └─Transformer: 2-2                                 --\n",
       "│    │    └─ModuleList: 3-5                             (42,527,232)\n",
       "├─Dropout: 1-2                                          --\n",
       "├─ReLU: 1-3                                             --\n",
       "├─Linear: 1-4                                           393,728\n",
       "├─Linear: 1-5                                           131,328\n",
       "├─Linear: 1-6                                           5,140\n",
       "├─LogSoftmax: 1-7                                       --\n",
       "================================================================================\n",
       "Total params: 66,893,076\n",
       "Trainable params: 530,196\n",
       "Non-trainable params: 66,362,880\n",
       "================================================================================"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# freeze all the parameters. This will prevent updating of model weights during fine-tuning.\n",
    "for param in bert.parameters():\n",
    "      param.requires_grad = False\n",
    "model = BERT_Arch(bert)\n",
    "# push the model to GPU\n",
    "# device = torch.device('cuda')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "70660442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "38bd168d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "# # from  sklearn.utils.compute_class_weight import compute_class_weight\n",
    "# #compute the class weights\n",
    "# class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
    "# print(class_wts)\n",
    "\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "class_wts = compute_class_weight(class_weight = \"balanced\", classes= np.unique(train_labels), y= train_labels)\n",
    "print(class_wts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fa5122a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class weights to tensor\n",
    "weights= torch.tensor(class_wts,dtype=torch.float)\n",
    "weights = weights.to(device)\n",
    "# loss function\n",
    "cross_entropy = nn.NLLLoss(weight=weights) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "520615ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6428e6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "# number of training epochs\n",
    "epochs = 200\n",
    "# We can also use learning rate scheduler to achieve better results\n",
    "lr_sch = lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a187d327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "  \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "    \n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step,    len(train_dataloader)))\n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch] \n",
    "        sent_id, mask, labels = batch\n",
    "#         print('labels',labels)\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "        # clip the the gradients to 1.0. It helps in preventing the    exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        # clear calculated gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # We are not using learning rate scheduler as of now\n",
    "        # lr_sch.step()\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "# compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ab56a7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 2 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 3 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 4 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 5 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 6 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 7 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 8 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 9 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 10 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 11 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 12 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 13 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 14 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 15 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 16 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 17 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 18 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 19 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 20 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 21 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 22 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 23 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 24 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 25 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 26 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 27 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 28 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 29 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 30 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 31 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 32 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 33 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 34 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 35 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 36 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 37 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 38 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 39 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 40 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 41 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 42 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 43 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 44 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 45 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 46 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 47 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 48 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 49 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 50 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 51 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 52 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 53 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 54 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 55 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 56 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 57 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 58 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 59 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 60 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 61 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 62 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 63 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 64 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 65 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 66 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 67 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 68 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 69 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 70 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 71 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 72 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 73 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 74 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 75 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 76 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 77 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 78 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 79 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 80 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 81 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 82 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 83 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 84 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 85 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 86 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 87 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 88 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 89 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 90 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 91 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 92 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 93 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 94 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 95 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 96 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 97 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 98 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 99 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 100 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 101 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 102 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 103 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 104 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 105 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 106 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 107 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 108 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 109 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 110 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 111 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 112 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 113 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 114 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 115 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 116 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 117 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 118 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 119 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 120 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 121 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 122 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 123 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 124 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 125 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 126 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 127 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 128 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 129 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 130 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 131 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 132 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 133 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 134 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 135 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 136 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 137 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 138 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 139 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 140 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 141 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 142 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 143 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 144 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 145 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 146 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 147 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 148 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 149 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 150 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 151 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 152 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 153 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 154 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 155 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 156 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 157 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 158 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 159 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 160 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 161 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 162 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 163 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 164 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 165 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 166 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 167 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 168 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 169 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 170 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 171 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 172 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 173 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 174 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 175 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 176 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 177 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 178 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 179 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 180 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 181 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 182 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 183 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 184 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 185 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 186 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 187 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 188 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 189 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 190 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 191 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 192 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 193 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 194 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 195 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 196 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 197 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 198 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 199 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      " Epoch 200 / 200\n",
      "  Batch    50  of    125.\n",
      "  Batch   100  of    125.\n",
      "\n",
      "Training Loss: 0.090\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    # it can make your experiment reproducible, similar to set  random seed to all options where there needs a random seed.\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "print(f'\\nTraining Loss: {train_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b03ee4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Open a file and use dump()\n",
    "with open('file_model.pkl', 'wb') as file:\n",
    "      \n",
    "    # A new file will be created\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "315f916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "  \n",
    "# Opening JSON file\n",
    "f = open('intent_text.json')\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67581157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a696fd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(str):\n",
    "    str = re.sub(r'[^a-zA-Z ]+', '', str)\n",
    "    test_text = [str]\n",
    "    model.eval()\n",
    "\n",
    "    tokens_test_data = tokenizer(\n",
    "    test_text,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    "    )\n",
    "    test_seq = torch.tensor(tokens_test_data['input_ids'])\n",
    "    test_mask = torch.tensor(tokens_test_data['attention_mask'])\n",
    "\n",
    "    preds = None\n",
    "    with torch.no_grad():\n",
    "        preds = model(test_seq.to(device), test_mask.to(device))\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        preds = np.argmax(preds, axis = 1)\n",
    "        print('Intent Identified: ', le.inverse_transform(preds)[0])\n",
    "        return le.inverse_transform(preds)[0]\n",
    "\n",
    "\n",
    "def get_response(message): \n",
    "    intent = get_prediction(message)\n",
    "    for i in data['intents']: \n",
    "        if i[\"tag\"] == intent:\n",
    "            result = random.choice(i[\"responses\"])\n",
    "            break\n",
    "    print(f\"Response : {result}\")\n",
    "    return \"Intent: \"+ intent + '\\n' + \"Response: \" + result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1d6d3356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent Identified:  credit_limit_change\n",
      "Response : you picked credit_limit_change\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Intent: credit_limit_change\\nResponse: you picked credit_limit_change'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response(\"increase my credit limit \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e66c825",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
